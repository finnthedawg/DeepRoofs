{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aerialBuildingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images and text. Of the form ./data/genTrain etc...\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.maps = os.listdir(root_dir) #These are all the maps in the root. austin10 chicago55 etc...\n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform \n",
    "\n",
    "        #Retrieve all the image locations with their coordinates\n",
    "        self.imageList = [] #List of images e.g ./data/genTrain/austin10/860.png\n",
    "        self.cornerList = [] #Get all the image paths in our dataset\n",
    "        for map in self.maps:\n",
    "            images = [x for x in os.listdir(os.path.join(root_dir,map)) if re.search(\"png\",x)]\n",
    "            coordinates = [x for x in os.listdir(os.path.join(root_dir,map)) if re.search(\"txt\",x)]\n",
    "            for image in images:\n",
    "                number = image.split('.png')[0]\n",
    "                corner = [x for x in coordinates if re.search('^'+number+'.txt',x)][0]\n",
    "                if len(corner) == 0:\n",
    "                    break\n",
    "                self.cornerList.append(os.path.join(root_dir, map,corner))\n",
    "                self.imageList.append(os.path.join(root_dir, map,image))\n",
    "        if (len(self.imageList) != len(self.cornerList)):\n",
    "            raise ValueError('Oops, mismatch in images and coordinate files')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imageList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = os.path.join(self.imageList[idx])\n",
    "        image = io.imread(img_file)\n",
    "        image = image[:,:,0:3]\n",
    "        corner_file = self.cornerList[idx]\n",
    "        with open(corner_file) as f:\n",
    "            content = f.readlines()\n",
    "            content = np.array([x.strip().split(' ') for x in content])\n",
    "        sample = {'image': image, 'corners': content}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to show building and its corners.\n",
    "def show_landmarks(image, corners):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    print(corners)\n",
    "    plt.scatter(corners[:, 0], corners[:, 1], s=10, marker='.', c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "building_dataset = aerialBuildingDataset(root_dir='./data/genTrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = building_dataset[1042]\n",
    "print(\"image shape\", sample['image'].shape, \"corner shape\", sample['corners'].shape)\n",
    "show_landmarks(**sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each input image, VGG-16 is used without tail layers as the backbone to extract a set of \"skip features\" 1/8 the size of input images. Then this goes through conv layers to obtain a heat-map mask of boundaries. This is then convolved to output a mask of candidate keypoints (V). Both B and V have size 1/8 the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamePad2d(nn.Module):\n",
    "    \"\"\"Mimics tensorflow's 'SAME' padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(SamePad2d, self).__init__()\n",
    "        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n",
    "        self.stride = torch.nn.modules.utils._pair(stride)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(input.shape)\n",
    "        in_width = input.size()[2]\n",
    "        in_height = input.size()[3]\n",
    "#         print(\"in_width\", in_width, \"and in_height\", in_height)\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n",
    "        pad_along_width = ((out_width - 1) * self.stride[0] +\n",
    "                           self.kernel_size[0] - in_width)\n",
    "        pad_along_height = ((out_height - 1) * self.stride[1] +\n",
    "                            self.kernel_size[1] - in_height)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network consisting of the skipfeature convolutions for generating 1/8 mask of candidate keypoints.\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, depth, pool_size, image_shape):\n",
    "        super(Mask, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.pool_size = pool_size\n",
    "        self.image_shape = image_shape\n",
    "        self.num_classes = 1\n",
    "        self.padding = SamePad2d(kernel_size=3, stride=1)\n",
    "        self.conv1 = nn.Conv2d(self.depth, 256, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256, eps=0.001)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256, eps=0.001)\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256, eps=0.001)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256, eps=0.001)\n",
    "#         self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, self.num_classes, kernel_size=1, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "#  We input individual buildings\n",
    "#         x = pyramid_roi_align([rois] + x, self.pool_size, self.image_shape)\n",
    "        print(\"Input to mask x is \", x.shape)\n",
    "        x = self.conv1(self.padding(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(self.padding(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(self.padding(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(self.padding(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.deconv(x)\n",
    "#         x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(\"Shape of x after foward\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #VGG net simplified.\n",
    "\n",
    "# class VGG1(nn.Module):\n",
    "#     def __init__(self, depth, pool_size, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoNet(nn.Module):\n",
    "    def __init__(self, model_dir):\n",
    "        \"\"\"\n",
    "        config: A Sub-class of the Config class\n",
    "        model_dir: Directory to save training logs and trained weights\n",
    "        \"\"\"\n",
    "        super(GeoNet, self).__init__()\n",
    "        self.MASK_POOL_SIZE = 288\n",
    "        self.IMAGE_SHAPE = 288\n",
    "        self.model_dir = model_dir\n",
    "        self.build()\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"Build GeoNet architecture.\n",
    "        \"\"\"\n",
    "#         Todo: Replace simple convolution of input to skip feature input. Using mask of 3 for 3 RGB\n",
    "#         self.backbone = Backbone(3)\n",
    "        self.mask = Mask(3, self.MASK_POOL_SIZE, self.IMAGE_SHAPE)\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize model weights.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def predict(self, input, mode):\n",
    "        molded_images = input\n",
    "\n",
    "        if mode == 'inference':\n",
    "            self.eval()\n",
    "        elif mode == 'training':\n",
    "            self.train()\n",
    "            # Set batchnorm always in eval mode during training\n",
    "            def set_bn_eval(m):\n",
    "                classname = m.__class__.__name__\n",
    "                if classname.find('BatchNorm') != -1:\n",
    "                    m.eval()\n",
    "            self.apply(set_bn_eval)\n",
    "            \n",
    "        #FPN and BBox extraction here.\n",
    "        mrcnn_feature_maps = molded_images\n",
    "\n",
    "        if mode == 'inference':\n",
    "            # Network Heads\n",
    "            # Create masks for detections\n",
    "            mrcnn_mask = self.mask(mrcnn_feature_maps)\n",
    "\n",
    "            # Add back batch dimension\n",
    "#             mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
    "            return mrcnn_mask\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeoNet(\n",
       "  (mask): Mask(\n",
       "    (padding): SamePad2d\n",
       "    (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn3): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn4): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = GeoNet('./placeholderDir')\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our inputBatch is of shape torch.Size([1, 3, 288, 288])\n",
      "Input to mask x is  torch.Size([1, 3, 288, 288])\n",
      "torch.Size([1, 3, 288, 288])\n",
      "in_width 288 and in_height 288\n",
      "torch.Size([1, 256, 288, 288])\n",
      "in_width 288 and in_height 288\n",
      "torch.Size([1, 256, 288, 288])\n",
      "in_width 288 and in_height 288\n",
      "torch.Size([1, 256, 288, 288])\n",
      "in_width 288 and in_height 288\n",
      "Shape of x after foward torch.Size([1, 1, 288, 288])\n"
     ]
    }
   ],
   "source": [
    "inputBatch = torch.Tensor(building_dataset[1032]['image'])\n",
    "inputBatch = torch.unsqueeze(inputBatch, 0)\n",
    "inputBatch = inputBatch.permute(0,3,1,2)\n",
    "print(\"Our inputBatch is of shape\", inputBatch.shape)\n",
    "CornerMask = net.predict(inputBatch,\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Take first batch dimension.\n",
    "show = PointMask[0].detach().numpy()\n",
    "#Squeeze the class dim.\n",
    "show = show.squeeze(0)\n",
    "\n",
    "# Z is your data set\n",
    "N = 288\n",
    "\n",
    "# G is a NxNx3 matrix\n",
    "G = np.zeros((N,N,3))\n",
    "\n",
    "# Where we set the RGB for each pixel\n",
    "G[show>0.9] = [1,1,1]\n",
    "G[show<0.9] = [0,0,0]\n",
    "\n",
    "plt.imshow(G,interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
